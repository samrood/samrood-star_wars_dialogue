{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation Using Keras\n",
    "This notebook will use the topics from each character in our set to create new lines of text based on the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load LSTM network and generate text\n",
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, TimeDistributed, Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Read in data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>dominant_topic</th>\n",
       "      <th>character</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Hello there!  Come here my little friend.  Don...</td>\n",
       "      <td>3</td>\n",
       "      <td>Obi-Wan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Don't worry, he'll be all right.</td>\n",
       "      <td>3</td>\n",
       "      <td>Obi-Wan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Rest easy, son, you've had a busy day.  You're...</td>\n",
       "      <td>2</td>\n",
       "      <td>Obi-Wan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The Jundland Wastes are not to be traveled lig...</td>\n",
       "      <td>0</td>\n",
       "      <td>Obi-Wan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Obi-Wan Kenobi... Obi-Wan?  Now thats a name I...</td>\n",
       "      <td>2</td>\n",
       "      <td>Obi-Wan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3483</th>\n",
       "      <td>3483</td>\n",
       "      <td>The code's changed. We need Artoo!</td>\n",
       "      <td>3</td>\n",
       "      <td>Leia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3484</th>\n",
       "      <td>3484</td>\n",
       "      <td>Artoo, where are you? We need you at the bunke...</td>\n",
       "      <td>5</td>\n",
       "      <td>Leia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3485</th>\n",
       "      <td>3485</td>\n",
       "      <td>I'll cover you.</td>\n",
       "      <td>3</td>\n",
       "      <td>Leia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3486</th>\n",
       "      <td>3486</td>\n",
       "      <td>It's not bad.</td>\n",
       "      <td>1</td>\n",
       "      <td>Leia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3487</th>\n",
       "      <td>3487</td>\n",
       "      <td>I know.</td>\n",
       "      <td>5</td>\n",
       "      <td>Leia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3488 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               text  \\\n",
       "0              0  Hello there!  Come here my little friend.  Don...   \n",
       "1              1                   Don't worry, he'll be all right.   \n",
       "2              2  Rest easy, son, you've had a busy day.  You're...   \n",
       "3              3  The Jundland Wastes are not to be traveled lig...   \n",
       "4              4  Obi-Wan Kenobi... Obi-Wan?  Now thats a name I...   \n",
       "...          ...                                                ...   \n",
       "3483        3483                 The code's changed. We need Artoo!   \n",
       "3484        3484  Artoo, where are you? We need you at the bunke...   \n",
       "3485        3485                                    I'll cover you.   \n",
       "3486        3486                                      It's not bad.   \n",
       "3487        3487                                            I know.   \n",
       "\n",
       "      dominant_topic character  \n",
       "0                  3   Obi-Wan  \n",
       "1                  3   Obi-Wan  \n",
       "2                  2   Obi-Wan  \n",
       "3                  0   Obi-Wan  \n",
       "4                  2   Obi-Wan  \n",
       "...              ...       ...  \n",
       "3483               3      Leia  \n",
       "3484               5      Leia  \n",
       "3485               3      Leia  \n",
       "3486               1      Leia  \n",
       "3487               5      Leia  \n",
       "\n",
       "[3488 rows x 4 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('character_topics.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we are going to seperate by character so can create text for one character at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_names = ['Obi-Wan', 'Vader', 'Luke', 'C-3PO', 'Han', 'Padme', 'Yoda', 'Anakin', 'Leia']\n",
    "characters_dict = {}\n",
    "\n",
    "obi_wan_df = df[df.character == 'Obi-Wan']\n",
    "obi_wan_df = obi_wan_df.drop(columns = ['character', 'Unnamed: 0'])\n",
    "obi_wan_df['num_words'] = obi_wan_df.apply(lambda x: len(x.text.split()), axis=1)\n",
    "characters_dict['Obi-Wan'] = obi_wan_df\n",
    "\n",
    "vader_df = df[df.character == 'Vader']\n",
    "vader_df = vader_df.drop(columns = ['character', 'Unnamed: 0']) \n",
    "vader_df['num_words'] = vader_df.apply(lambda x: len(x.text.split()), axis=1)\n",
    "characters_dict['Vader'] = vader_df\n",
    "\n",
    "luke_df = df[df.character == 'Luke']\n",
    "luke_df = luke_df.drop(columns = ['character', 'Unnamed: 0']) \n",
    "luke_df['num_words'] = luke_df.apply(lambda x: len(x.text.split()), axis=1)\n",
    "characters_dict['Luke'] = luke_df\n",
    "\n",
    "c_3po_df = df[df.character == 'C-3PO']\n",
    "c_3po_df = c_3po_df.drop(columns = ['character', 'Unnamed: 0'])\n",
    "c_3po_df['num_words'] = c_3po_df.apply(lambda x: len(x.text.split()), axis=1)\n",
    "characters_dict['C-3PO'] = c_3po_df\n",
    "\n",
    "han_df = df[df.character == 'Han']\n",
    "han_df = han_df.drop(columns = ['character', 'Unnamed: 0']) \n",
    "han_df['num_words'] = han_df.apply(lambda x: len(x.text.split()), axis=1)\n",
    "characters_dict['Han'] = han_df\n",
    "\n",
    "padme_df = df[df.character == 'Padme']\n",
    "padme_df = padme_df.drop(columns = ['character', 'Unnamed: 0'])\n",
    "padme_df['num_words'] = padme_df.apply(lambda x: len(x.text.split()), axis=1)\n",
    "characters_dict['Padme'] = padme_df\n",
    "\n",
    "yoda_df = df[df.character == 'Yoda']\n",
    "yoda_df = yoda_df.drop(columns = ['character', 'Unnamed: 0']) \n",
    "yoda_df['num_words'] = yoda_df.apply(lambda x: len(x.text.split()), axis=1)\n",
    "characters_dict['Yoda'] = yoda_df\n",
    "\n",
    "anakin_df = df[df.character == 'Anakin']\n",
    "anakin_df = anakin_df.drop(columns = ['character', 'Unnamed: 0']) \n",
    "anakin_df['num_words'] = anakin_df.apply(lambda x: len(x.text.split()), axis=1)\n",
    "characters_dict['Anakin'] = anakin_df\n",
    "\n",
    "leia_df = df[df.character == 'Leia']\n",
    "leia_df = leia_df.drop(columns = ['character', 'Unnamed: 0']) \n",
    "leia_df['num_words'] = leia_df.apply(lambda x: len(x.text.split()), axis=1)\n",
    "characters_dict['Leia'] = leia_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>dominant_topic</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello there!  Come here my little friend.  Don...</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Don't worry, he'll be all right.</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rest easy, son, you've had a busy day.  You're...</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Jundland Wastes are not to be traveled lig...</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Obi-Wan Kenobi... Obi-Wan?  Now thats a name I...</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>I will take the child and watch over him. Mast...</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>Training??</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>Who?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>Qui-Gon? But, how could he accomplish this?</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>I will be able to talk with him?</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>589 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  dominant_topic  \\\n",
       "0    Hello there!  Come here my little friend.  Don...               3   \n",
       "1                     Don't worry, he'll be all right.               3   \n",
       "2    Rest easy, son, you've had a busy day.  You're...               2   \n",
       "3    The Jundland Wastes are not to be traveled lig...               0   \n",
       "4    Obi-Wan Kenobi... Obi-Wan?  Now thats a name I...               2   \n",
       "..                                                 ...             ...   \n",
       "584  I will take the child and watch over him. Mast...               1   \n",
       "585                                         Training??               6   \n",
       "586                                               Who?               0   \n",
       "587        Qui-Gon? But, how could he accomplish this?               1   \n",
       "588                   I will be able to talk with him?               4   \n",
       "\n",
       "     num_words  \n",
       "0           10  \n",
       "1            6  \n",
       "2           15  \n",
       "3           19  \n",
       "4           17  \n",
       "..         ...  \n",
       "584         23  \n",
       "585          1  \n",
       "586          1  \n",
       "587          7  \n",
       "588          8  \n",
       "\n",
       "[589 rows x 3 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters_dict['Obi-Wan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generation \n",
    "Using a dictionary to save the new lines generated, we will loop thorugh all the charachters then all their topics and save their new lines to be read into a flask app. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_names = ['Obi-Wan', 'Anakin', 'Vader', 'Luke', 'Padme', 'Leia', 'C-3PO', 'Han', 'Yoda']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  8797\n",
      "Total Vocab:  29\n",
      "Total Patterns:  8697\n",
      "Epoch 1/100\n",
      "8697/8697 [==============================] - 81s 9ms/step - loss: 2.9183\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.91830, saving model to weight.best.hdf5\n",
      "Epoch 2/100\n",
      "8697/8697 [==============================] - 80s 9ms/step - loss: 2.8336\n",
      "\n",
      "Epoch 00002: loss improved from 2.91830 to 2.83361, saving model to weight.best.hdf5\n",
      "Epoch 3/100\n",
      "8697/8697 [==============================] - 94s 11ms/step - loss: 2.8287\n",
      "\n",
      "Epoch 00003: loss improved from 2.83361 to 2.82872, saving model to weight.best.hdf5\n",
      "Epoch 4/100\n",
      "8697/8697 [==============================] - 81s 9ms/step - loss: 2.8226\n",
      "\n",
      "Epoch 00004: loss improved from 2.82872 to 2.82265, saving model to weight.best.hdf5\n",
      "Epoch 5/100\n",
      "8697/8697 [==============================] - 72s 8ms/step - loss: 2.8155\n",
      "\n",
      "Epoch 00005: loss improved from 2.82265 to 2.81548, saving model to weight.best.hdf5\n",
      "Epoch 6/100\n",
      "8697/8697 [==============================] - 75s 9ms/step - loss: 2.8029\n",
      "\n",
      "Epoch 00006: loss improved from 2.81548 to 2.80287, saving model to weight.best.hdf5\n",
      "Epoch 7/100\n",
      "8697/8697 [==============================] - 79s 9ms/step - loss: 2.7790\n",
      "\n",
      "Epoch 00007: loss improved from 2.80287 to 2.77900, saving model to weight.best.hdf5\n",
      "Epoch 8/100\n",
      "8697/8697 [==============================] - 95s 11ms/step - loss: 2.7464\n",
      "\n",
      "Epoch 00008: loss improved from 2.77900 to 2.74638, saving model to weight.best.hdf5\n",
      "Epoch 9/100\n",
      "8697/8697 [==============================] - 87s 10ms/step - loss: 2.7159\n",
      "\n",
      "Epoch 00009: loss improved from 2.74638 to 2.71591, saving model to weight.best.hdf5\n",
      "Epoch 10/100\n",
      "8697/8697 [==============================] - 74s 9ms/step - loss: 2.7012\n",
      "\n",
      "Epoch 00010: loss improved from 2.71591 to 2.70116, saving model to weight.best.hdf5\n",
      "Epoch 11/100\n",
      "8697/8697 [==============================] - 71s 8ms/step - loss: 2.6820\n",
      "\n",
      "Epoch 00011: loss improved from 2.70116 to 2.68198, saving model to weight.best.hdf5\n",
      "Epoch 12/100\n",
      "8697/8697 [==============================] - 74s 8ms/step - loss: 2.6630\n",
      "\n",
      "Epoch 00012: loss improved from 2.68198 to 2.66303, saving model to weight.best.hdf5\n",
      "Epoch 13/100\n",
      "8697/8697 [==============================] - 73s 8ms/step - loss: 2.6499\n",
      "\n",
      "Epoch 00013: loss improved from 2.66303 to 2.64989, saving model to weight.best.hdf5\n",
      "Epoch 14/100\n",
      "8697/8697 [==============================] - 76s 9ms/step - loss: 2.6289\n",
      "\n",
      "Epoch 00014: loss improved from 2.64989 to 2.62887, saving model to weight.best.hdf5\n",
      "Epoch 15/100\n",
      "8697/8697 [==============================] - 73s 8ms/step - loss: 2.6084\n",
      "\n",
      "Epoch 00015: loss improved from 2.62887 to 2.60836, saving model to weight.best.hdf5\n",
      "Epoch 16/100\n",
      "8697/8697 [==============================] - 67s 8ms/step - loss: 2.5929\n",
      "\n",
      "Epoch 00016: loss improved from 2.60836 to 2.59289, saving model to weight.best.hdf5\n",
      "Epoch 17/100\n",
      "8697/8697 [==============================] - 72s 8ms/step - loss: 2.5785\n",
      "\n",
      "Epoch 00017: loss improved from 2.59289 to 2.57848, saving model to weight.best.hdf5\n",
      "Epoch 18/100\n",
      "8697/8697 [==============================] - 68s 8ms/step - loss: 2.5601\n",
      "\n",
      "Epoch 00018: loss improved from 2.57848 to 2.56005, saving model to weight.best.hdf5\n",
      "Epoch 19/100\n",
      "8697/8697 [==============================] - 71s 8ms/step - loss: 2.5437\n",
      "\n",
      "Epoch 00019: loss improved from 2.56005 to 2.54366, saving model to weight.best.hdf5\n",
      "Epoch 20/100\n",
      "8697/8697 [==============================] - 71s 8ms/step - loss: 2.5295\n",
      "\n",
      "Epoch 00020: loss improved from 2.54366 to 2.52946, saving model to weight.best.hdf5\n",
      "Epoch 21/100\n",
      "8697/8697 [==============================] - 73s 8ms/step - loss: 2.5127\n",
      "\n",
      "Epoch 00021: loss improved from 2.52946 to 2.51274, saving model to weight.best.hdf5\n",
      "Epoch 22/100\n",
      "8697/8697 [==============================] - 68s 8ms/step - loss: 2.4805\n",
      "\n",
      "Epoch 00022: loss improved from 2.51274 to 2.48049, saving model to weight.best.hdf5\n",
      "Epoch 23/100\n",
      "8697/8697 [==============================] - 69s 8ms/step - loss: 2.4572\n",
      "\n",
      "Epoch 00023: loss improved from 2.48049 to 2.45719, saving model to weight.best.hdf5\n",
      "Epoch 24/100\n",
      "8697/8697 [==============================] - 73s 8ms/step - loss: 2.4279\n",
      "\n",
      "Epoch 00024: loss improved from 2.45719 to 2.42794, saving model to weight.best.hdf5\n",
      "Epoch 25/100\n",
      "8697/8697 [==============================] - 65s 7ms/step - loss: 2.3981\n",
      "\n",
      "Epoch 00025: loss improved from 2.42794 to 2.39806, saving model to weight.best.hdf5\n",
      "Epoch 26/100\n",
      "8697/8697 [==============================] - 69s 8ms/step - loss: 2.3556\n",
      "\n",
      "Epoch 00026: loss improved from 2.39806 to 2.35558, saving model to weight.best.hdf5\n",
      "Epoch 27/100\n",
      "8697/8697 [==============================] - 67s 8ms/step - loss: 2.3203\n",
      "\n",
      "Epoch 00027: loss improved from 2.35558 to 2.32032, saving model to weight.best.hdf5\n",
      "Epoch 28/100\n",
      "8697/8697 [==============================] - 72s 8ms/step - loss: 2.2650\n",
      "\n",
      "Epoch 00028: loss improved from 2.32032 to 2.26500, saving model to weight.best.hdf5\n",
      "Epoch 29/100\n",
      "8697/8697 [==============================] - 74s 8ms/step - loss: 2.2149\n",
      "\n",
      "Epoch 00029: loss improved from 2.26500 to 2.21493, saving model to weight.best.hdf5\n",
      "Epoch 30/100\n",
      "8697/8697 [==============================] - 69s 8ms/step - loss: 2.1472\n",
      "\n",
      "Epoch 00030: loss improved from 2.21493 to 2.14718, saving model to weight.best.hdf5\n",
      "Epoch 31/100\n",
      "8697/8697 [==============================] - 73s 8ms/step - loss: 2.0745\n",
      "\n",
      "Epoch 00031: loss improved from 2.14718 to 2.07451, saving model to weight.best.hdf5\n",
      "Epoch 32/100\n",
      "8697/8697 [==============================] - 73s 8ms/step - loss: 1.9889\n",
      "\n",
      "Epoch 00032: loss improved from 2.07451 to 1.98893, saving model to weight.best.hdf5\n",
      "Epoch 33/100\n",
      "8697/8697 [==============================] - 71s 8ms/step - loss: 1.8952\n",
      "\n",
      "Epoch 00033: loss improved from 1.98893 to 1.89519, saving model to weight.best.hdf5\n",
      "Epoch 34/100\n",
      "8697/8697 [==============================] - 70s 8ms/step - loss: 1.8051\n",
      "\n",
      "Epoch 00034: loss improved from 1.89519 to 1.80510, saving model to weight.best.hdf5\n",
      "Epoch 35/100\n",
      "8697/8697 [==============================] - 69s 8ms/step - loss: 1.6885\n",
      "\n",
      "Epoch 00035: loss improved from 1.80510 to 1.68854, saving model to weight.best.hdf5\n",
      "Epoch 36/100\n",
      "8697/8697 [==============================] - 71s 8ms/step - loss: 1.5612\n",
      "\n",
      "Epoch 00036: loss improved from 1.68854 to 1.56121, saving model to weight.best.hdf5\n",
      "Epoch 37/100\n",
      "8697/8697 [==============================] - 74s 8ms/step - loss: 1.4392\n",
      "\n",
      "Epoch 00037: loss improved from 1.56121 to 1.43917, saving model to weight.best.hdf5\n",
      "Epoch 38/100\n",
      "8697/8697 [==============================] - 71s 8ms/step - loss: 1.2961\n",
      "\n",
      "Epoch 00038: loss improved from 1.43917 to 1.29613, saving model to weight.best.hdf5\n",
      "Epoch 39/100\n",
      "8697/8697 [==============================] - 69s 8ms/step - loss: 1.1562\n",
      "\n",
      "Epoch 00039: loss improved from 1.29613 to 1.15617, saving model to weight.best.hdf5\n",
      "Epoch 40/100\n",
      "8697/8697 [==============================] - 73s 8ms/step - loss: 1.0074\n",
      "\n",
      "Epoch 00040: loss improved from 1.15617 to 1.00745, saving model to weight.best.hdf5\n",
      "Epoch 41/100\n",
      "8697/8697 [==============================] - 70s 8ms/step - loss: 0.8440\n",
      "\n",
      "Epoch 00041: loss improved from 1.00745 to 0.84402, saving model to weight.best.hdf5\n",
      "Epoch 42/100\n",
      "8697/8697 [==============================] - 76s 9ms/step - loss: 0.7192\n",
      "\n",
      "Epoch 00042: loss improved from 0.84402 to 0.71925, saving model to weight.best.hdf5\n",
      "Epoch 43/100\n",
      "8697/8697 [==============================] - 75s 9ms/step - loss: 0.5869\n",
      "\n",
      "Epoch 00043: loss improved from 0.71925 to 0.58689, saving model to weight.best.hdf5\n",
      "Epoch 44/100\n",
      "8697/8697 [==============================] - 65s 7ms/step - loss: 0.4796\n",
      "\n",
      "Epoch 00044: loss improved from 0.58689 to 0.47961, saving model to weight.best.hdf5\n",
      "Epoch 45/100\n",
      "8697/8697 [==============================] - 64s 7ms/step - loss: 0.3882\n",
      "\n",
      "Epoch 00045: loss improved from 0.47961 to 0.38821, saving model to weight.best.hdf5\n",
      "Epoch 46/100\n",
      "8697/8697 [==============================] - 73s 8ms/step - loss: 0.3014\n",
      "\n",
      "Epoch 00046: loss improved from 0.38821 to 0.30138, saving model to weight.best.hdf5\n",
      "Epoch 47/100\n",
      "8697/8697 [==============================] - 70s 8ms/step - loss: 0.2397\n",
      "\n",
      "Epoch 00047: loss improved from 0.30138 to 0.23971, saving model to weight.best.hdf5\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8697/8697 [==============================] - 73s 8ms/step - loss: 0.1830\n",
      "\n",
      "Epoch 00048: loss improved from 0.23971 to 0.18297, saving model to weight.best.hdf5\n",
      "Epoch 49/100\n",
      "8697/8697 [==============================] - 66s 8ms/step - loss: 0.1458\n",
      "\n",
      "Epoch 00049: loss improved from 0.18297 to 0.14582, saving model to weight.best.hdf5\n",
      "Epoch 50/100\n",
      "8697/8697 [==============================] - 67s 8ms/step - loss: 0.1193\n",
      "\n",
      "Epoch 00050: loss improved from 0.14582 to 0.11927, saving model to weight.best.hdf5\n",
      "Epoch 51/100\n",
      "8697/8697 [==============================] - 66s 8ms/step - loss: 0.0913\n",
      "\n",
      "Epoch 00051: loss improved from 0.11927 to 0.09130, saving model to weight.best.hdf5\n",
      "Epoch 52/100\n",
      "8697/8697 [==============================] - 68s 8ms/step - loss: 0.0756\n",
      "\n",
      "Epoch 00052: loss improved from 0.09130 to 0.07564, saving model to weight.best.hdf5\n",
      "Epoch 53/100\n",
      "8697/8697 [==============================] - 65s 7ms/step - loss: 0.0704\n",
      "\n",
      "Epoch 00053: loss improved from 0.07564 to 0.07035, saving model to weight.best.hdf5\n",
      "Epoch 54/100\n",
      "8697/8697 [==============================] - 65s 7ms/step - loss: 0.0576\n",
      "\n",
      "Epoch 00054: loss improved from 0.07035 to 0.05757, saving model to weight.best.hdf5\n",
      "Epoch 55/100\n",
      "8697/8697 [==============================] - 66s 8ms/step - loss: 0.0514\n",
      "\n",
      "Epoch 00055: loss improved from 0.05757 to 0.05143, saving model to weight.best.hdf5\n",
      "Epoch 56/100\n",
      "8697/8697 [==============================] - 72s 8ms/step - loss: 0.0410\n",
      "\n",
      "Epoch 00056: loss improved from 0.05143 to 0.04101, saving model to weight.best.hdf5\n",
      "Epoch 57/100\n",
      "8697/8697 [==============================] - 69s 8ms/step - loss: 0.0334\n",
      "\n",
      "Epoch 00057: loss improved from 0.04101 to 0.03341, saving model to weight.best.hdf5\n",
      "Epoch 58/100\n",
      "8697/8697 [==============================] - 78s 9ms/step - loss: 0.0358\n",
      "\n",
      "Epoch 00058: loss did not improve from 0.03341\n",
      "Epoch 59/100\n",
      "8697/8697 [==============================] - 76s 9ms/step - loss: 0.0374\n",
      "\n",
      "Epoch 00059: loss did not improve from 0.03341\n",
      "Epoch 60/100\n",
      "8697/8697 [==============================] - 72s 8ms/step - loss: 0.0308\n",
      "\n",
      "Epoch 00060: loss improved from 0.03341 to 0.03082, saving model to weight.best.hdf5\n",
      "Epoch 61/100\n",
      "8697/8697 [==============================] - 78s 9ms/step - loss: 0.0261\n",
      "\n",
      "Epoch 00061: loss improved from 0.03082 to 0.02614, saving model to weight.best.hdf5\n",
      "Epoch 62/100\n",
      "8697/8697 [==============================] - 72s 8ms/step - loss: 0.0221\n",
      "\n",
      "Epoch 00062: loss improved from 0.02614 to 0.02212, saving model to weight.best.hdf5\n",
      "Epoch 63/100\n",
      "8697/8697 [==============================] - 72s 8ms/step - loss: 0.0217\n",
      "\n",
      "Epoch 00063: loss improved from 0.02212 to 0.02174, saving model to weight.best.hdf5\n",
      "Epoch 64/100\n",
      "8697/8697 [==============================] - 71s 8ms/step - loss: 3.3310\n",
      "\n",
      "Epoch 00064: loss did not improve from 0.02174\n",
      "Epoch 65/100\n",
      "8697/8697 [==============================] - 71s 8ms/step - loss: 3.3301\n",
      "\n",
      "Epoch 00065: loss did not improve from 0.02174\n",
      "Epoch 66/100\n",
      "8697/8697 [==============================] - 74s 9ms/step - loss: 3.1660\n",
      "\n",
      "Epoch 00066: loss did not improve from 0.02174\n",
      "Epoch 67/100\n",
      "8697/8697 [==============================] - 73s 8ms/step - loss: 3.0611\n",
      "\n",
      "Epoch 00067: loss did not improve from 0.02174\n",
      "Epoch 68/100\n",
      "8697/8697 [==============================] - 77s 9ms/step - loss: 2.9805\n",
      "\n",
      "Epoch 00068: loss did not improve from 0.02174\n",
      "Epoch 69/100\n",
      "8697/8697 [==============================] - 65s 8ms/step - loss: 2.9336\n",
      "\n",
      "Epoch 00069: loss did not improve from 0.02174\n",
      "Epoch 70/100\n",
      "8697/8697 [==============================] - 73s 8ms/step - loss: 2.8750\n",
      "\n",
      "Epoch 00070: loss did not improve from 0.02174\n",
      "Epoch 71/100\n",
      "8697/8697 [==============================] - 73s 8ms/step - loss: 2.8523\n",
      "\n",
      "Epoch 00071: loss did not improve from 0.02174\n",
      "Epoch 72/100\n",
      "8697/8697 [==============================] - 70s 8ms/step - loss: 2.8113\n",
      "\n",
      "Epoch 00072: loss did not improve from 0.02174\n",
      "Epoch 73/100\n",
      "8697/8697 [==============================] - 87s 10ms/step - loss: 2.7884\n",
      "\n",
      "Epoch 00073: loss did not improve from 0.02174\n",
      "Epoch 74/100\n",
      "8697/8697 [==============================] - 66s 8ms/step - loss: 2.7576\n",
      "\n",
      "Epoch 00074: loss did not improve from 0.02174\n",
      "Epoch 75/100\n",
      "8697/8697 [==============================] - 70s 8ms/step - loss: 2.7456\n",
      "\n",
      "Epoch 00075: loss did not improve from 0.02174\n",
      "Epoch 76/100\n",
      "8697/8697 [==============================] - 75s 9ms/step - loss: 2.7278\n",
      "\n",
      "Epoch 00076: loss did not improve from 0.02174\n",
      "Epoch 77/100\n",
      "8697/8697 [==============================] - 74s 9ms/step - loss: 2.7131\n",
      "\n",
      "Epoch 00077: loss did not improve from 0.02174\n",
      "Epoch 78/100\n",
      "8697/8697 [==============================] - 70s 8ms/step - loss: 2.6962\n",
      "\n",
      "Epoch 00078: loss did not improve from 0.02174\n",
      "Epoch 79/100\n",
      "8697/8697 [==============================] - 79s 9ms/step - loss: 2.6889\n",
      "\n",
      "Epoch 00079: loss did not improve from 0.02174\n",
      "Epoch 80/100\n",
      "8697/8697 [==============================] - 83s 10ms/step - loss: 2.6796\n",
      "\n",
      "Epoch 00080: loss did not improve from 0.02174\n",
      "Epoch 81/100\n",
      "8697/8697 [==============================] - 77s 9ms/step - loss: 2.6705\n",
      "\n",
      "Epoch 00081: loss did not improve from 0.02174\n",
      "Epoch 82/100\n",
      "8697/8697 [==============================] - 79s 9ms/step - loss: 2.6560\n",
      "\n",
      "Epoch 00082: loss did not improve from 0.02174\n",
      "Epoch 83/100\n",
      "8697/8697 [==============================] - 78s 9ms/step - loss: 2.6530\n",
      "\n",
      "Epoch 00083: loss did not improve from 0.02174\n",
      "Epoch 84/100\n",
      "8697/8697 [==============================] - 73s 8ms/step - loss: 2.6343\n",
      "\n",
      "Epoch 00084: loss did not improve from 0.02174\n",
      "Epoch 85/100\n",
      "8697/8697 [==============================] - 72s 8ms/step - loss: 2.6254\n",
      "\n",
      "Epoch 00085: loss did not improve from 0.02174\n",
      "Epoch 86/100\n",
      "8697/8697 [==============================] - 74s 9ms/step - loss: 2.6112\n",
      "\n",
      "Epoch 00086: loss did not improve from 0.02174\n",
      "Epoch 87/100\n",
      "8697/8697 [==============================] - 72s 8ms/step - loss: 2.6018\n",
      "\n",
      "Epoch 00087: loss did not improve from 0.02174\n",
      "Epoch 88/100\n",
      "8697/8697 [==============================] - 77s 9ms/step - loss: 2.5791\n",
      "\n",
      "Epoch 00088: loss did not improve from 0.02174\n",
      "Epoch 89/100\n",
      "8697/8697 [==============================] - 75s 9ms/step - loss: 2.5622\n",
      "\n",
      "Epoch 00089: loss did not improve from 0.02174\n",
      "Epoch 90/100\n",
      "8697/8697 [==============================] - 75s 9ms/step - loss: 2.5439\n",
      "\n",
      "Epoch 00090: loss did not improve from 0.02174\n",
      "Epoch 91/100\n",
      "8697/8697 [==============================] - 75s 9ms/step - loss: 2.5282\n",
      "\n",
      "Epoch 00091: loss did not improve from 0.02174\n",
      "Epoch 92/100\n",
      "8697/8697 [==============================] - 76s 9ms/step - loss: 2.4979\n",
      "\n",
      "Epoch 00092: loss did not improve from 0.02174\n",
      "Epoch 93/100\n",
      "8697/8697 [==============================] - 73s 8ms/step - loss: 2.4655\n",
      "\n",
      "Epoch 00093: loss did not improve from 0.02174\n",
      "Epoch 94/100\n",
      "8697/8697 [==============================] - 77s 9ms/step - loss: 2.4173\n",
      "\n",
      "Epoch 00094: loss did not improve from 0.02174\n",
      "Epoch 95/100\n",
      "8697/8697 [==============================] - 79s 9ms/step - loss: 2.3606\n",
      "\n",
      "Epoch 00095: loss did not improve from 0.02174\n",
      "Epoch 96/100\n",
      "8697/8697 [==============================] - 71s 8ms/step - loss: 2.2610\n",
      "\n",
      "Epoch 00096: loss did not improve from 0.02174\n",
      "Epoch 97/100\n",
      "8697/8697 [==============================] - 87s 10ms/step - loss: 2.1718\n",
      "\n",
      "Epoch 00097: loss did not improve from 0.02174\n",
      "Epoch 98/100\n",
      "8697/8697 [==============================] - 76s 9ms/step - loss: 1.9859\n",
      "\n",
      "Epoch 00098: loss did not improve from 0.02174\n",
      "Epoch 99/100\n",
      "8697/8697 [==============================] - 83s 10ms/step - loss: 1.7953\n",
      "\n",
      "Epoch 00099: loss did not improve from 0.02174\n",
      "Epoch 100/100\n",
      "8697/8697 [==============================] - 85s 10ms/step - loss: 1.5230\n",
      "\n",
      "Epoch 00100: loss did not improve from 0.02174\n",
      "Seed:\n",
      "\" u   i   h a v e   t r a i n e d   y o u   s i n c e   y o u   w e r e   a   s m a l l   b o y   i   h a v e   t a u g h t   y o u   e v e r y t h i n g   i   k n o w   a n d   y o u   h a v e   b e c \"\n",
      "\n",
      "::::\n",
      "\n",
      "ome a far greater jedi than i could ever hope to be and you have saved my life more time than i can remember but be patient anakin it will not be long before the council make you a jedi master do not worry i have enough clone with me to take three system the size of utapau i think i will be able to handle the situation even without your help emergency code nine thirteen i have no contact on any frequency are there any jedi out there anywhere how many other jedi managed to survive it would be bet\n",
      "Seed:\n",
      "\" n   a n a k i n   w i t h o u t   t h e   a p p r o v a l   o f   t h e   c o u n c i l   i f   i   m u s t   i   a m   s u r e   t h e   j e d i   c o u n c i l   h a v e   t h e i r   r e a s o n   \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "::::\n",
      "\n",
      "we are here to protect you senator not to start an investigation what captain typho ha more than enough men downstairs no assassin will try that way any activity up here it is not an intruder i am worried about there are many other why to kill a senator you are using her a bait because of your mother mind your thought anakin they betray you you have made a commitment to the jedi order a commitment not easily broken and do not forget she is a politician they are not to be trusted that wa too clos\n",
      "Seed:\n",
      "\" t   m a s t e r   y o u   c o u l d   b e   s i t t i n g   o n   t h e   c o u n c i l   b y   n o w   i f   y o u   w o u l d   j u s t   f o l l o w   t h e   c o d e   t h e y   w i l l   n o t   \"\n",
      "\n",
      "::::\n",
      "\n",
      "go along with you this time iam ready to face the trial jar jar is on his way to the gungan city master no master yoda igave qui-gon my word i will train anakin without the approval of the council if i must i am sure the jedi council have their reason we are here to protect you senator not to start an investigation what captain typho ha more than enough men downstairs no assassin will try that way any activity up here it is not an intruder i am worried about there are many other why to kill a se\n",
      "Seed:\n",
      "\" m i s s i o n   w h a t   w a   i t   m a s t e r   y o u   c o u l d   b e   s i t t i n g   o n   t h e   c o u n c i l   b y   n o w   i f   y o u   w o u l d   j u s t   f o l l o w   t h e   c o \"\n",
      "\n",
      "::::\n",
      "\n",
      "de they will not go along with you this time iam ready to face the trial jar jar is on his way to the gungan city master no master yoda igave qui-gon my word i will train anakin without the approval of the council if i must i am sure the jedi council have their reason we are here to protect you senator not to start an investigation what captain typho ha more than enough men downstairs no assassin will try that way any activity up here it is not an intruder i am worried about there are many other\n",
      "Seed:\n",
      "\" e p   m e   s t e a d y   h o l d   o n   n o t   y e t   n o w   b r e a k   l e f t   y o u   y o u   w i l l   n e v e r   g e t   t h r o u g h   t h e r e   a n a k i n   i t   i s   t o o   t i \"\n",
      "\n",
      "::::\n",
      "\n",
      "ght i am hit anakin anakin hold your fire hold your fire you are not helping here anakin six droids coming our way we will never get through that it is too small anakin this is no time for joke we are in serious trouble here that will not hold when the fuel hit those power dischargers no did i say anything i did not say anything did i miss something what is that steady attitude eighteen degree steady steady the endless speech anakin let u be fair today you are the hero and you deserve your glori\n",
      "Seed:\n",
      "\" r f u l   a n d   l e i a   f o l l o w i n g   i n   h e r   f o s t e r   f a t h e r   ' s   p a t h   b e c a m e   a   s e n a t o r   a   w e l l   t h a t   i s   n o t   a l l   s h e   b e c \"\n",
      "\n",
      "::::\n",
      "\n",
      "ame of course she became the leader of her cell in the alliance against the corrupt empire and because she had diplomatic immunity she wa a vital link for getting information to the rebel cause that is what she wa doing when her path crossed yours for her foster parent had always told her to contact me on tatooine if her trouble became desperate she ha not been trained in the way of the jedi the way you have luke but the force is strong with her a it is with all of your family there is no avoidi\n",
      "Seed:\n",
      "\"   m e   t o   t a k e   t h r e e   s y s t e m   t h e   s i z e   o f   u t a p a u   i   t h i n k   i   w i l l   b e   a b l e   t o   h a n d l e   t h e   s i t u a t i o n   e v e n   w i t h \"\n",
      "\n",
      "::::\n",
      "\n",
      "out your help emergency code nine thirteen i have no contact on any frequency are there any jedi out there anywhere how many other jedi managed to survive it would be better if we stayed with the senator there are several battalion of clone trooper on every level many are dressed a jedi i 've recalibrated the code warning all surviving jedi to stay away it can not be it can not be send me to kill the emperor i will not kill anakin ha anakin been here the republic ha fallen padme the jedi order i\n",
      "Seed:\n",
      "\" t h e   d a u g h t e r   o f   s e n a t o r   o r g a n o n   o n   a l d e r a a n   t h e   o r g a n o n   h o u s e h o l d   w a   h i g h - b o r n   a n d   p o l i t i c a l l y   q u i t e \"\n",
      "\n",
      "::::\n",
      "\n",
      " powerful in that system leia became a princess by virtue of lineage no one knew she would been adopted of course but it wa a title without real power since alderaan had long been a democracy even so the family continued to be politically powerful and leia following in her foster father 's path became a senator a well that is not all she became of course she became the leader of her cell in the alliance against the corrupt empire and because she had diplomatic immunity she wa a vital link for ge\n",
      "Seed:\n",
      "\" e   w i t h   m e   t o   t a k e   t h r e e   s y s t e m   t h e   s i z e   o f   u t a p a u   i   t h i n k   i   w i l l   b e   a b l e   t o   h a n d l e   t h e   s i t u a t i o n   e v e \"\n",
      "\n",
      "::::\n",
      "\n",
      "n without your help emergency code nine thirteen i have no contact on any frequency are there any jedi out there anywhere how many other jedi managed to survive it would be better if we stayed with the senator there are several battalion of clone trooper on every level many are dressed a jedi i 've recalibrated the code warning all surviving jedi to stay away it can not be it can not be send me to kill the emperor i will not kill anakin ha anakin been here the republic ha fallen padme the jedi o\n",
      "Seed:\n",
      "\" s t e m   d o e s n   t   s h o w   u p   o n   t h e   a r c h i v e   m a p   m a s t e r   h e   s h o u l d   n o t   h a v e   b e e n   g i v e n   t h i s   a s s i g n m e n t   i   a m   a f \"\n",
      "\n",
      "::::\n",
      "\n",
      "raid anakin will not be able to protect the senator he ha a an emotional connection with her it is been there since he wa boy now he is confused distracted that is why i am here unaltered and these the repubic arfour arfour i 've never heard of a jedi called tidn-dyas have you master i have a strong feeling that this bounty hunter is the assassin we are looking for your clone are very impressive you must be very proud are not we all ever made your way a far into the interior a coruscant no i tho\n",
      "Seed:\n",
      "\" v e r y   p r o u d   a r e   n o t   w e   a l l   e v e r   m a d e   y o u r   w a y   a   f a r   i n t o   t h e   i n t e r i o r   a   c o r u s c a n t   n o   i   t h o u g h t   i   w i l l \"\n",
      "\n",
      "::::\n",
      "\n",
      " not forget i wa beginning to wonder if you had gotten my message hold on look over there no forget her we have to go after dooku follow that speeder anakin no anakin i must admit without the clone it would not have been a victory anakin they are all over me just keep me steady hold on not yet now break left you you will never get through there anakin it is too tight i am hit anakin anakin hold your fire hold your fire you are not helping here anakin six droids coming our way we will never get t\n",
      "Seed:\n",
      "\" a k i n   t h e   f a c t   o f   t h e   m a t t e r   i s   y o u   a r e   t o o   c l o s e   t o   t h e   c h a n c e l l o r   t h e   c o u n c i l   d o e   n o t   l i k e   i t   w h e n   \"\n",
      "\n",
      "::::\n",
      "\n",
      "he interferes in jedi affair no it is not anakin i worry when you speak of jealousy and pride those are not jedi thought they are dangerous dark thought anakin the only reason the council ha approved your appointment is because the chancellor trust you this assignment is not to be on record the council asked me to approach you on this personally that is why you must help u anakin our allegiance is to the senate not to it leader who ha managed to stay in office long after his term ha expired the \n",
      "Seed:\n",
      "\" r   h a p p e n e d   b e f o r e   l i s t e n   t o   m e   a n a k i n   t h e   f a c t   o f   t h e   m a t t e r   i s   y o u   a r e   t o o   c l o s e   t o   t h e   c h a n c e l l o r   \"\n",
      "\n",
      "::::\n",
      "\n",
      "the council doe not like it when he interferes in jedi affair no it is not anakin i worry when you speak of jealousy and pride those are not jedi thought they are dangerous dark thought anakin the only reason the council ha approved your appointment is because the chancellor trust you this assignment is not to be on record the council asked me to approach you on this personally that is why you must help u anakin our allegiance is to the senate not to it leader who ha managed to stay in office lo\n",
      "Seed:\n",
      "\" e r   w a y   t o   k i l l   a   s e n a t o r   y o u   a r e   u s i n g   h e r   a   b a i t   b e c a u s e   o f   y o u r   m o t h e r   m i n d   y o u r   t h o u g h t   a n a k i n   t h \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "::::\n",
      "\n",
      "ey betray you you have made a commitment to the jedi order a commitment not easily broken and do not forget she is a politician they are not to be trusted that wa too close what it wa stupid where are you going he went down there the other way anakin this weapon is your life but you have not learned anything anakin toxic dart it is a toxic dart i need to know where it came from and who made it i am looking for dexter he is not in trouble it is personal i never understood why he quit only twenty \n",
      "Seed:\n",
      "\" h   t h e   p o l i t i c i a n   a n a k i n   b e   c a r e f u l   o f   y o u r   f r i e n d   p a l p a t i n e   h e   h a   r e q u e s t e d   y o u r   p r e s e n c e   c a l m   d o w n   \"\n",
      "\n",
      "::::\n",
      "\n",
      "anakin you have been given a great honor to be on the council at your age it is never happened before listen to me anakin the fact of the matter is you are too close to the chancellor the council doe not like it when he interferes in jedi affair no it is not anakin i worry when you speak of jealousy and pride those are not jedi thought they are dangerous dark thought anakin the only reason the council ha approved your appointment is because the chancellor trust you this assignment is not to be o\n",
      "Seed:\n",
      "\" t o l d   h e r   t o   c o n t a c t   m e   o n   t a t o o i n e   i f   h e r   t r o u b l e   b e c a m e   d e s p e r a t e   s h e   h a   n o t   b e e n   t r a i n e d   i n   t h e   w a \"\n",
      "\n",
      "::::\n",
      "\n",
      "y of the jedi the way you have luke but the force is strong with her a it is with all of your family there is no avoiding the battle you must face and destroy vader it is not about the mission master it is somethging elsewhere elusive sorry master the water fried my weapon what is this they banished you because you are clumsy you overdid it what is it that wa close now stay here and keep out of trouble here master tatooine it is small out ef thtl not le alone whe cmpiro you must learn the way of\n",
      "Seed:\n",
      "\"   i m m u n i t y   s h e   w a   a   v i t a l   l i n k   f o r   g e t t i n g   i n f o r m a t i o n   t o   t h e   r e b e l   c a u s e   t h a t   i s   w h a t   s h e   w a   d o i n g   w \"\n",
      "\n",
      "::::\n",
      "\n",
      "hen her path crossed yours for her foster parent had always told her to contact me on tatooine if her trouble became desperate she ha not been trained in the way of the jedi the way you have luke but the force is strong with her a it is with all of your family there is no avoiding the battle you must face and destroy vader it is not about the mission master it is somethging elsewhere elusive sorry master the water fried my weapon what is this they banished you because you are clumsy you overdid \n",
      "Seed:\n",
      "\" n l y   r e a s o n   t h e   c o u n c i l   h a   a p p r o v e d   y o u r   a p p o i n t m e n t   i s   b e c a u s e   t h e   c h a n c e l l o r   t r u s t   y o u   t h i s   a s s i g n m \"\n",
      "\n",
      "::::\n",
      "\n",
      "ent is not to be on record the council asked me to approach you on this personally that is why you must help u anakin our allegiance is to the senate not to it leader who ha managed to stay in office long after his term ha expired the council is asking you he will not let me down he never ha ha anakin been to see you you should be a jedi padme you are strong and wise anakin and i am very proud of you i have trained you since you were a small boy i have taught you everything i know and you have b\n",
      "Seed:\n",
      "\" a t o o i n e   a n d   y o u r   m o t h e r   t o o k   l e i a   t o   l i v e   a   t h e   d a u g h t e r   o f   s e n a t o r   o r g a n o n   o n   a l d e r a a n   t h e   o r g a n o n   \"\n",
      "\n",
      "::::\n",
      "\n",
      "household wa high-born and politically quite powerful in that system leia became a princess by virtue of lineage no one knew she would been adopted of course but it wa a title without real power since alderaan had long been a democracy even so the family continued to be politically powerful and leia following in her foster father 's path became a senator a well that is not all she became of course she became the leader of her cell in the alliance against the corrupt empire and because she had di\n",
      "Seed:\n",
      "\"   b e   a b l e   t o   h a n d l e   t h e   s i t u a t i o n   e v e n   w i t h o u t   y o u r   h e l p   e m e r g e n c y   c o d e   n i n e   t h i r t e e n   i   h a v e   n o   c o n t a \"\n",
      "\n",
      "::::\n",
      "\n",
      "ct on any frequency are there any jedi out there anywhere how many other jedi managed to survive it would be better if we stayed with the senator there are several battalion of clone trooper on every level many are dressed a jedi i 've recalibrated the code warning all surviving jedi to stay away it can not be it can not be send me to kill the emperor i will not kill anakin ha anakin been here the republic ha fallen padme the jedi order is no more i am here looking for anakin when wa the last ti\n",
      "['ome a far greater jedi than i could ever hope to be and you have saved my life more time than i can remember but be patient anakin it will not be long before the council make you a jedi master do not worry i have enough clone with me to take three system the size of utapau i think i will be able to handle the situation even without your help emergency code nine thirteen i have no contact on any frequency are there any jedi out there anywhere how many other jedi managed to survive it would be bet', 'we are here to protect you senator not to start an investigation what captain typho ha more than enough men downstairs no assassin will try that way any activity up here it is not an intruder i am worried about there are many other why to kill a senator you are using her a bait because of your mother mind your thought anakin they betray you you have made a commitment to the jedi order a commitment not easily broken and do not forget she is a politician they are not to be trusted that wa too clos', 'go along with you this time iam ready to face the trial jar jar is on his way to the gungan city master no master yoda igave qui-gon my word i will train anakin without the approval of the council if i must i am sure the jedi council have their reason we are here to protect you senator not to start an investigation what captain typho ha more than enough men downstairs no assassin will try that way any activity up here it is not an intruder i am worried about there are many other why to kill a se', 'de they will not go along with you this time iam ready to face the trial jar jar is on his way to the gungan city master no master yoda igave qui-gon my word i will train anakin without the approval of the council if i must i am sure the jedi council have their reason we are here to protect you senator not to start an investigation what captain typho ha more than enough men downstairs no assassin will try that way any activity up here it is not an intruder i am worried about there are many other', 'ght i am hit anakin anakin hold your fire hold your fire you are not helping here anakin six droids coming our way we will never get through that it is too small anakin this is no time for joke we are in serious trouble here that will not hold when the fuel hit those power dischargers no did i say anything i did not say anything did i miss something what is that steady attitude eighteen degree steady steady the endless speech anakin let u be fair today you are the hero and you deserve your glori', 'ame of course she became the leader of her cell in the alliance against the corrupt empire and because she had diplomatic immunity she wa a vital link for getting information to the rebel cause that is what she wa doing when her path crossed yours for her foster parent had always told her to contact me on tatooine if her trouble became desperate she ha not been trained in the way of the jedi the way you have luke but the force is strong with her a it is with all of your family there is no avoidi', \"out your help emergency code nine thirteen i have no contact on any frequency are there any jedi out there anywhere how many other jedi managed to survive it would be better if we stayed with the senator there are several battalion of clone trooper on every level many are dressed a jedi i 've recalibrated the code warning all surviving jedi to stay away it can not be it can not be send me to kill the emperor i will not kill anakin ha anakin been here the republic ha fallen padme the jedi order i\", \" powerful in that system leia became a princess by virtue of lineage no one knew she would been adopted of course but it wa a title without real power since alderaan had long been a democracy even so the family continued to be politically powerful and leia following in her foster father 's path became a senator a well that is not all she became of course she became the leader of her cell in the alliance against the corrupt empire and because she had diplomatic immunity she wa a vital link for ge\", \"n without your help emergency code nine thirteen i have no contact on any frequency are there any jedi out there anywhere how many other jedi managed to survive it would be better if we stayed with the senator there are several battalion of clone trooper on every level many are dressed a jedi i 've recalibrated the code warning all surviving jedi to stay away it can not be it can not be send me to kill the emperor i will not kill anakin ha anakin been here the republic ha fallen padme the jedi o\", \"raid anakin will not be able to protect the senator he ha a an emotional connection with her it is been there since he wa boy now he is confused distracted that is why i am here unaltered and these the repubic arfour arfour i 've never heard of a jedi called tidn-dyas have you master i have a strong feeling that this bounty hunter is the assassin we are looking for your clone are very impressive you must be very proud are not we all ever made your way a far into the interior a coruscant no i tho\", ' not forget i wa beginning to wonder if you had gotten my message hold on look over there no forget her we have to go after dooku follow that speeder anakin no anakin i must admit without the clone it would not have been a victory anakin they are all over me just keep me steady hold on not yet now break left you you will never get through there anakin it is too tight i am hit anakin anakin hold your fire hold your fire you are not helping here anakin six droids coming our way we will never get t', 'he interferes in jedi affair no it is not anakin i worry when you speak of jealousy and pride those are not jedi thought they are dangerous dark thought anakin the only reason the council ha approved your appointment is because the chancellor trust you this assignment is not to be on record the council asked me to approach you on this personally that is why you must help u anakin our allegiance is to the senate not to it leader who ha managed to stay in office long after his term ha expired the ', 'the council doe not like it when he interferes in jedi affair no it is not anakin i worry when you speak of jealousy and pride those are not jedi thought they are dangerous dark thought anakin the only reason the council ha approved your appointment is because the chancellor trust you this assignment is not to be on record the council asked me to approach you on this personally that is why you must help u anakin our allegiance is to the senate not to it leader who ha managed to stay in office lo', 'ey betray you you have made a commitment to the jedi order a commitment not easily broken and do not forget she is a politician they are not to be trusted that wa too close what it wa stupid where are you going he went down there the other way anakin this weapon is your life but you have not learned anything anakin toxic dart it is a toxic dart i need to know where it came from and who made it i am looking for dexter he is not in trouble it is personal i never understood why he quit only twenty ', 'anakin you have been given a great honor to be on the council at your age it is never happened before listen to me anakin the fact of the matter is you are too close to the chancellor the council doe not like it when he interferes in jedi affair no it is not anakin i worry when you speak of jealousy and pride those are not jedi thought they are dangerous dark thought anakin the only reason the council ha approved your appointment is because the chancellor trust you this assignment is not to be o', 'y of the jedi the way you have luke but the force is strong with her a it is with all of your family there is no avoiding the battle you must face and destroy vader it is not about the mission master it is somethging elsewhere elusive sorry master the water fried my weapon what is this they banished you because you are clumsy you overdid it what is it that wa close now stay here and keep out of trouble here master tatooine it is small out ef thtl not le alone whe cmpiro you must learn the way of', 'hen her path crossed yours for her foster parent had always told her to contact me on tatooine if her trouble became desperate she ha not been trained in the way of the jedi the way you have luke but the force is strong with her a it is with all of your family there is no avoiding the battle you must face and destroy vader it is not about the mission master it is somethging elsewhere elusive sorry master the water fried my weapon what is this they banished you because you are clumsy you overdid ', 'ent is not to be on record the council asked me to approach you on this personally that is why you must help u anakin our allegiance is to the senate not to it leader who ha managed to stay in office long after his term ha expired the council is asking you he will not let me down he never ha ha anakin been to see you you should be a jedi padme you are strong and wise anakin and i am very proud of you i have trained you since you were a small boy i have taught you everything i know and you have b', \"household wa high-born and politically quite powerful in that system leia became a princess by virtue of lineage no one knew she would been adopted of course but it wa a title without real power since alderaan had long been a democracy even so the family continued to be politically powerful and leia following in her foster father 's path became a senator a well that is not all she became of course she became the leader of her cell in the alliance against the corrupt empire and because she had di\", \"ct on any frequency are there any jedi out there anywhere how many other jedi managed to survive it would be better if we stayed with the senator there are several battalion of clone trooper on every level many are dressed a jedi i 've recalibrated the code warning all surviving jedi to stay away it can not be it can not be send me to kill the emperor i will not kill anakin ha anakin been here the republic ha fallen padme the jedi order is no more i am here looking for anakin when wa the last ti\"]\n",
      "\n",
      "Done.\n",
      "Total Characters:  8797\n",
      "Total Vocab:  29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  8697\n",
      "Epoch 1/100\n",
      "8697/8697 [==============================] - 71s 8ms/step - loss: 2.9132\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.91322, saving model to weight.best.hdf5\n",
      "Epoch 2/100\n",
      "8697/8697 [==============================] - 76s 9ms/step - loss: 2.8354\n",
      "\n",
      "Epoch 00002: loss improved from 2.91322 to 2.83542, saving model to weight.best.hdf5\n",
      "Epoch 3/100\n",
      "8697/8697 [==============================] - 96s 11ms/step - loss: 2.8304\n",
      "\n",
      "Epoch 00003: loss improved from 2.83542 to 2.83042, saving model to weight.best.hdf5\n",
      "Epoch 4/100\n",
      "8697/8697 [==============================] - 91s 11ms/step - loss: 2.8302\n",
      "\n",
      "Epoch 00004: loss improved from 2.83042 to 2.83023, saving model to weight.best.hdf5\n",
      "Epoch 5/100\n",
      "8697/8697 [==============================] - 100s 11ms/step - loss: 2.8246\n",
      "\n",
      "Epoch 00005: loss improved from 2.83023 to 2.82464, saving model to weight.best.hdf5\n",
      "Epoch 6/100\n",
      "8697/8697 [==============================] - 102s 12ms/step - loss: 2.8155\n",
      "\n",
      "Epoch 00006: loss improved from 2.82464 to 2.81549, saving model to weight.best.hdf5\n",
      "Epoch 7/100\n",
      "8697/8697 [==============================] - 95s 11ms/step - loss: 2.8056\n",
      "\n",
      "Epoch 00007: loss improved from 2.81549 to 2.80557, saving model to weight.best.hdf5\n",
      "Epoch 8/100\n",
      "8697/8697 [==============================] - 87s 10ms/step - loss: 2.7882\n",
      "\n",
      "Epoch 00008: loss improved from 2.80557 to 2.78819, saving model to weight.best.hdf5\n",
      "Epoch 9/100\n",
      "8697/8697 [==============================] - 82s 9ms/step - loss: 2.7608\n",
      "\n",
      "Epoch 00009: loss improved from 2.78819 to 2.76082, saving model to weight.best.hdf5\n",
      "Epoch 10/100\n",
      "8697/8697 [==============================] - 85s 10ms/step - loss: 2.7288\n",
      "\n",
      "Epoch 00010: loss improved from 2.76082 to 2.72879, saving model to weight.best.hdf5\n",
      "Epoch 11/100\n",
      "8697/8697 [==============================] - 137s 16ms/step - loss: 2.7114\n",
      "\n",
      "Epoch 00011: loss improved from 2.72879 to 2.71139, saving model to weight.best.hdf5\n",
      "Epoch 12/100\n",
      "8697/8697 [==============================] - 243s 28ms/step - loss: 2.6857\n",
      "\n",
      "Epoch 00012: loss improved from 2.71139 to 2.68571, saving model to weight.best.hdf5\n",
      "Epoch 13/100\n",
      "8697/8697 [==============================] - 242s 28ms/step - loss: 2.6659\n",
      "\n",
      "Epoch 00013: loss improved from 2.68571 to 2.66591, saving model to weight.best.hdf5\n",
      "Epoch 14/100\n",
      "8697/8697 [==============================] - 235s 27ms/step - loss: 2.6409\n",
      "\n",
      "Epoch 00014: loss improved from 2.66591 to 2.64086, saving model to weight.best.hdf5\n",
      "Epoch 15/100\n",
      "8697/8697 [==============================] - 222s 26ms/step - loss: 2.6270\n",
      "\n",
      "Epoch 00015: loss improved from 2.64086 to 2.62705, saving model to weight.best.hdf5\n",
      "Epoch 16/100\n",
      "8697/8697 [==============================] - 217s 25ms/step - loss: 2.6115\n",
      "\n",
      "Epoch 00016: loss improved from 2.62705 to 2.61148, saving model to weight.best.hdf5\n",
      "Epoch 17/100\n",
      "8697/8697 [==============================] - 229s 26ms/step - loss: 2.6004\n",
      "\n",
      "Epoch 00017: loss improved from 2.61148 to 2.60036, saving model to weight.best.hdf5\n",
      "Epoch 18/100\n",
      "8697/8697 [==============================] - 233s 27ms/step - loss: 2.5838\n",
      "\n",
      "Epoch 00018: loss improved from 2.60036 to 2.58380, saving model to weight.best.hdf5\n",
      "Epoch 19/100\n",
      "8697/8697 [==============================] - 237s 27ms/step - loss: 2.5675\n",
      "\n",
      "Epoch 00019: loss improved from 2.58380 to 2.56747, saving model to weight.best.hdf5\n",
      "Epoch 20/100\n",
      "8697/8697 [==============================] - 234s 27ms/step - loss: 2.5511\n",
      "\n",
      "Epoch 00020: loss improved from 2.56747 to 2.55108, saving model to weight.best.hdf5\n",
      "Epoch 21/100\n",
      "8697/8697 [==============================] - 232s 27ms/step - loss: 2.5359\n",
      "\n",
      "Epoch 00021: loss improved from 2.55108 to 2.53590, saving model to weight.best.hdf5\n",
      "Epoch 22/100\n",
      "8697/8697 [==============================] - 233s 27ms/step - loss: 2.5118\n",
      "\n",
      "Epoch 00022: loss improved from 2.53590 to 2.51182, saving model to weight.best.hdf5\n",
      "Epoch 23/100\n",
      "8697/8697 [==============================] - 224s 26ms/step - loss: 2.4847\n",
      "\n",
      "Epoch 00023: loss improved from 2.51182 to 2.48473, saving model to weight.best.hdf5\n",
      "Epoch 24/100\n",
      "8697/8697 [==============================] - 192s 22ms/step - loss: 2.4648\n",
      "\n",
      "Epoch 00024: loss improved from 2.48473 to 2.46481, saving model to weight.best.hdf5\n",
      "Epoch 25/100\n",
      "8697/8697 [==============================] - 198s 23ms/step - loss: 2.4320\n",
      "\n",
      "Epoch 00025: loss improved from 2.46481 to 2.43200, saving model to weight.best.hdf5\n",
      "Epoch 26/100\n",
      "8697/8697 [==============================] - 196s 23ms/step - loss: 2.3942\n",
      "\n",
      "Epoch 00026: loss improved from 2.43200 to 2.39416, saving model to weight.best.hdf5\n",
      "Epoch 27/100\n",
      "8697/8697 [==============================] - 240s 28ms/step - loss: 2.3676\n",
      "\n",
      "Epoch 00027: loss improved from 2.39416 to 2.36757, saving model to weight.best.hdf5\n",
      "Epoch 28/100\n",
      "8697/8697 [==============================] - 184s 21ms/step - loss: 2.3186\n",
      "\n",
      "Epoch 00028: loss improved from 2.36757 to 2.31864, saving model to weight.best.hdf5\n",
      "Epoch 29/100\n",
      "8697/8697 [==============================] - 178s 20ms/step - loss: 2.2775\n",
      "\n",
      "Epoch 00029: loss improved from 2.31864 to 2.27748, saving model to weight.best.hdf5\n",
      "Epoch 30/100\n",
      "8697/8697 [==============================] - 176s 20ms/step - loss: 2.2296\n",
      "\n",
      "Epoch 00030: loss improved from 2.27748 to 2.22964, saving model to weight.best.hdf5\n",
      "Epoch 31/100\n",
      "8697/8697 [==============================] - 173s 20ms/step - loss: 2.1434\n",
      "\n",
      "Epoch 00031: loss improved from 2.22964 to 2.14343, saving model to weight.best.hdf5\n",
      "Epoch 32/100\n",
      "8697/8697 [==============================] - 533s 61ms/step - loss: 2.0746\n",
      "\n",
      "Epoch 00032: loss improved from 2.14343 to 2.07462, saving model to weight.best.hdf5\n",
      "Epoch 33/100\n",
      "8697/8697 [==============================] - 173s 20ms/step - loss: 2.0013\n",
      "\n",
      "Epoch 00033: loss improved from 2.07462 to 2.00133, saving model to weight.best.hdf5\n",
      "Epoch 34/100\n",
      "8697/8697 [==============================] - 175s 20ms/step - loss: 1.9001\n",
      "\n",
      "Epoch 00034: loss improved from 2.00133 to 1.90012, saving model to weight.best.hdf5\n",
      "Epoch 35/100\n",
      "8697/8697 [==============================] - 173s 20ms/step - loss: 1.7982\n",
      "\n",
      "Epoch 00035: loss improved from 1.90012 to 1.79823, saving model to weight.best.hdf5\n",
      "Epoch 36/100\n",
      "8697/8697 [==============================] - 174s 20ms/step - loss: 1.6726\n",
      "\n",
      "Epoch 00036: loss improved from 1.79823 to 1.67259, saving model to weight.best.hdf5\n",
      "Epoch 37/100\n",
      "8697/8697 [==============================] - 174s 20ms/step - loss: 1.5523\n",
      "\n",
      "Epoch 00037: loss improved from 1.67259 to 1.55233, saving model to weight.best.hdf5\n",
      "Epoch 38/100\n",
      "8697/8697 [==============================] - 1351s 155ms/step - loss: 1.4156\n",
      "\n",
      "Epoch 00038: loss improved from 1.55233 to 1.41564, saving model to weight.best.hdf5\n",
      "Epoch 39/100\n",
      "8697/8697 [==============================] - 174s 20ms/step - loss: 1.2684\n",
      "\n",
      "Epoch 00039: loss improved from 1.41564 to 1.26839, saving model to weight.best.hdf5\n",
      "Epoch 40/100\n",
      "8697/8697 [==============================] - 174s 20ms/step - loss: 1.1169\n",
      "\n",
      "Epoch 00040: loss improved from 1.26839 to 1.11686, saving model to weight.best.hdf5\n",
      "Epoch 41/100\n",
      "8697/8697 [==============================] - 178s 20ms/step - loss: 0.9683\n",
      "\n",
      "Epoch 00041: loss improved from 1.11686 to 0.96831, saving model to weight.best.hdf5\n",
      "Epoch 42/100\n",
      "8697/8697 [==============================] - 185s 21ms/step - loss: 0.8304\n",
      "\n",
      "Epoch 00042: loss improved from 0.96831 to 0.83038, saving model to weight.best.hdf5\n",
      "Epoch 43/100\n",
      "8697/8697 [==============================] - 174s 20ms/step - loss: 0.7010\n",
      "\n",
      "Epoch 00043: loss improved from 0.83038 to 0.70102, saving model to weight.best.hdf5\n",
      "Epoch 44/100\n",
      "8697/8697 [==============================] - 175s 20ms/step - loss: 0.5627\n",
      "\n",
      "Epoch 00044: loss improved from 0.70102 to 0.56271, saving model to weight.best.hdf5\n",
      "Epoch 45/100\n",
      "8697/8697 [==============================] - 174s 20ms/step - loss: 0.4567\n",
      "\n",
      "Epoch 00045: loss improved from 0.56271 to 0.45668, saving model to weight.best.hdf5\n",
      "Epoch 46/100\n",
      "8697/8697 [==============================] - 176s 20ms/step - loss: 0.3751\n",
      "\n",
      "Epoch 00046: loss improved from 0.45668 to 0.37508, saving model to weight.best.hdf5\n",
      "Epoch 47/100\n",
      "8697/8697 [==============================] - 200s 23ms/step - loss: 0.2961\n",
      "\n",
      "Epoch 00047: loss improved from 0.37508 to 0.29605, saving model to weight.best.hdf5\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8697/8697 [==============================] - 210s 24ms/step - loss: 0.2313\n",
      "\n",
      "Epoch 00048: loss improved from 0.29605 to 0.23129, saving model to weight.best.hdf5\n",
      "Epoch 49/100\n",
      "8697/8697 [==============================] - 151s 17ms/step - loss: 0.1785\n",
      "\n",
      "Epoch 00049: loss improved from 0.23129 to 0.17847, saving model to weight.best.hdf5\n",
      "Epoch 50/100\n",
      "8697/8697 [==============================] - 145s 17ms/step - loss: 0.1431\n",
      "\n",
      "Epoch 00050: loss improved from 0.17847 to 0.14314, saving model to weight.best.hdf5\n",
      "Epoch 51/100\n",
      "8697/8697 [==============================] - 132s 15ms/step - loss: 0.1125\n",
      "\n",
      "Epoch 00051: loss improved from 0.14314 to 0.11247, saving model to weight.best.hdf5\n",
      "Epoch 52/100\n",
      "8697/8697 [==============================] - 181s 21ms/step - loss: 0.0949\n",
      "\n",
      "Epoch 00052: loss improved from 0.11247 to 0.09489, saving model to weight.best.hdf5\n",
      "Epoch 53/100\n",
      "8697/8697 [==============================] - 224s 26ms/step - loss: 0.0726\n",
      "\n",
      "Epoch 00053: loss improved from 0.09489 to 0.07257, saving model to weight.best.hdf5\n",
      "Epoch 54/100\n",
      "8697/8697 [==============================] - 215s 25ms/step - loss: 0.0589\n",
      "\n",
      "Epoch 00054: loss improved from 0.07257 to 0.05886, saving model to weight.best.hdf5\n",
      "Epoch 55/100\n",
      "8697/8697 [==============================] - 207s 24ms/step - loss: 0.0525\n",
      "\n",
      "Epoch 00055: loss improved from 0.05886 to 0.05252, saving model to weight.best.hdf5\n",
      "Epoch 56/100\n",
      "8697/8697 [==============================] - 198s 23ms/step - loss: 0.0460\n",
      "\n",
      "Epoch 00056: loss improved from 0.05252 to 0.04601, saving model to weight.best.hdf5\n",
      "Epoch 57/100\n",
      "8697/8697 [==============================] - 189s 22ms/step - loss: 0.0379\n",
      "\n",
      "Epoch 00057: loss improved from 0.04601 to 0.03789, saving model to weight.best.hdf5\n",
      "Epoch 58/100\n",
      "8697/8697 [==============================] - 2856s 328ms/step - loss: 0.0461\n",
      "\n",
      "Epoch 00058: loss did not improve from 0.03789\n",
      "Epoch 59/100\n",
      "8697/8697 [==============================] - 197s 23ms/step - loss: 2.2251\n",
      "\n",
      "Epoch 00059: loss did not improve from 0.03789\n",
      "Epoch 60/100\n",
      "8697/8697 [==============================] - 205s 24ms/step - loss: 2.4737\n",
      "\n",
      "Epoch 00060: loss did not improve from 0.03789\n",
      "Epoch 61/100\n",
      "8697/8697 [==============================] - 188s 22ms/step - loss: 2.0439\n",
      "\n",
      "Epoch 00061: loss did not improve from 0.03789\n",
      "Epoch 62/100\n",
      "8697/8697 [==============================] - 197s 23ms/step - loss: 1.5165\n",
      "\n",
      "Epoch 00062: loss did not improve from 0.03789\n",
      "Epoch 63/100\n",
      "8697/8697 [==============================] - 193s 22ms/step - loss: 0.9855\n",
      "\n",
      "Epoch 00063: loss did not improve from 0.03789\n",
      "Epoch 64/100\n",
      "8697/8697 [==============================] - 213s 24ms/step - loss: 0.6205\n",
      "\n",
      "Epoch 00064: loss did not improve from 0.03789\n",
      "Epoch 65/100\n",
      "8697/8697 [==============================] - 192s 22ms/step - loss: 0.3666\n",
      "\n",
      "Epoch 00065: loss did not improve from 0.03789\n",
      "Epoch 66/100\n",
      "8697/8697 [==============================] - 195s 22ms/step - loss: 0.2277\n",
      "\n",
      "Epoch 00066: loss did not improve from 0.03789\n",
      "Epoch 67/100\n",
      "8697/8697 [==============================] - 190s 22ms/step - loss: 0.1607\n",
      "\n",
      "Epoch 00067: loss did not improve from 0.03789\n",
      "Epoch 68/100\n",
      "8697/8697 [==============================] - 186s 21ms/step - loss: 0.1147\n",
      "\n",
      "Epoch 00068: loss did not improve from 0.03789\n",
      "Epoch 69/100\n",
      "8697/8697 [==============================] - 187s 21ms/step - loss: 0.0885\n",
      "\n",
      "Epoch 00069: loss did not improve from 0.03789\n",
      "Epoch 70/100\n",
      "8697/8697 [==============================] - 203s 23ms/step - loss: 0.0742\n",
      "\n",
      "Epoch 00070: loss did not improve from 0.03789\n",
      "Epoch 71/100\n",
      "8697/8697 [==============================] - 208s 24ms/step - loss: 0.0580\n",
      "\n",
      "Epoch 00071: loss did not improve from 0.03789\n",
      "Epoch 72/100\n",
      "8697/8697 [==============================] - 194s 22ms/step - loss: 0.0518\n",
      "\n",
      "Epoch 00072: loss did not improve from 0.03789\n",
      "Epoch 73/100\n",
      "8697/8697 [==============================] - 180s 21ms/step - loss: 0.0556\n",
      "\n",
      "Epoch 00073: loss did not improve from 0.03789\n",
      "Epoch 74/100\n",
      "8697/8697 [==============================] - 179s 21ms/step - loss: 0.1228\n",
      "\n",
      "Epoch 00074: loss did not improve from 0.03789\n",
      "Epoch 75/100\n",
      "8697/8697 [==============================] - 176s 20ms/step - loss: 0.0871\n",
      "\n",
      "Epoch 00075: loss did not improve from 0.03789\n",
      "Epoch 76/100\n",
      "8697/8697 [==============================] - 203s 23ms/step - loss: 0.0463\n",
      "\n",
      "Epoch 00076: loss did not improve from 0.03789\n",
      "Epoch 77/100\n",
      "8697/8697 [==============================] - 186s 21ms/step - loss: 0.0340\n",
      "\n",
      "Epoch 00077: loss improved from 0.03789 to 0.03402, saving model to weight.best.hdf5\n",
      "Epoch 78/100\n",
      "8697/8697 [==============================] - 178s 20ms/step - loss: 0.0274\n",
      "\n",
      "Epoch 00078: loss improved from 0.03402 to 0.02739, saving model to weight.best.hdf5\n",
      "Epoch 79/100\n",
      "6528/8697 [=====================>........] - ETA: 40s - loss: 0.0240"
     ]
    }
   ],
   "source": [
    "quotes_dict = {}\n",
    "for character in character_names_first:\n",
    "    quote_list = []\n",
    "    \n",
    "    # run thorugh each topic cluster \n",
    "    for cluster in range(0,8): \n",
    "        # get the text for that cluster \n",
    "        raw_data = characters_dict[character]\n",
    "        clustered_data = raw_data.loc[raw_data['dominant_topic'] == cluster]['text']\n",
    "        raw_text = clustered_data.to_list()\n",
    "        raw_text = ' '.join(data)\n",
    "        # map characters to integers (and back) in order to do modeling\n",
    "        chars = sorted(list(set(raw_text)))\n",
    "        char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "        int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "        \n",
    "        # take a peek at what we are working wirh \n",
    "        n_chars = len(raw_text)\n",
    "        n_vocab = len(chars)\n",
    "        print(\"Total Characters: \", n_chars)\n",
    "        print(\"Total Vocab: \", n_vocab)\n",
    "        \n",
    "        # prepare the dataset of input to output pairs encoded as integers\n",
    "        seq_length = 100\n",
    "        dataX = []\n",
    "        dataY = []\n",
    "        for i in range(0, n_chars - seq_length, 1):\n",
    "            seq_in = raw_text[i:i + seq_length]\n",
    "            seq_out = raw_text[i + seq_length]\n",
    "            dataX.append([char_to_int[char] for char in seq_in])\n",
    "            dataY.append(char_to_int[seq_out])\n",
    "        n_patterns = len(dataX)\n",
    "        print(\"Total Patterns: \", n_patterns)\n",
    "        \n",
    "        # reshape the input code and normalize it\n",
    "        X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "        X = X / float(n_vocab)\n",
    "        # one hot encode the output variable\n",
    "        y = np_utils.to_categorical(dataY)\n",
    "        \n",
    "        # define lstem model\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(500, input_shape=(X.shape[1], X.shape[2])))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "        # load the weights for the model \n",
    "        filepath = 'weight.best.hdf5'\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint]\n",
    "        \n",
    "        # fit the model\n",
    "        model.fit(X, y, epochs=100, batch_size=128, callbacks=callbacks_list)\n",
    "        model.load_weights(filepath)\n",
    "\n",
    "        \n",
    "        # generate new text \n",
    "        quotes = []\n",
    "        # makeing 20 quotes (for time, can make more)\n",
    "        for i in range(0,20):\n",
    "            # pick a random seed \n",
    "            start = numpy.random.randint(0, len(dataX)-1)\n",
    "            pattern = dataX[start]\n",
    "            # look at random seed that model will predict on \n",
    "            print(\"Seed:\")\n",
    "            print(\"\\\"\", ' '.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "            \n",
    "            # generate characters\n",
    "            text = []\n",
    "            for i in range(500):\n",
    "                x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "                x = x / float(n_vocab)\n",
    "                prediction = model.predict(x, verbose=0)\n",
    "                index = numpy.argmax(prediction)\n",
    "                result = int_to_char[index]\n",
    "                seq_in = [int_to_char[value] for value in pattern]\n",
    "                pattern.append(index)\n",
    "                pattern = pattern[1:len(pattern)]\n",
    "                text.append(result)\n",
    "            print(\"\\n::::\\n\")\n",
    "            # save the new quotes in a dictionary for that character topic \n",
    "            text_string = ''.join([str(elem) for elem in text])\n",
    "            quotes.append(text_string)\n",
    "            print(text_string)\n",
    "        quote_list.append(quotes)\n",
    "        print(quotes)\n",
    "        print(\"\\nDone.\")\n",
    "    quotes_dict[character] = quote_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('star-wars-app/quotes.pickle', 'wb') as handle:\n",
    "    pickle.dump(quotes_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = pickle.load(open(\"star-wars-app/quotes.pickle\", \"rb\" ))\n",
    "dict2 = pickle.load(open(\"star-wars-app/quotes.pickle\", \"rb\" ))\n",
    "# Merge contents of dict2 in dict1\n",
    "dict1.update(dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict3 = pickle.load(open(\"star-wars-app/quotes.pickle\", \"rb\" ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
